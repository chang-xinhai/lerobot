#!/usr/bin/env python

# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""3D Diffusion Policy (DP3) implementation for LeRobot.

Based on "3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple 3D Representations"
(paper: https://arxiv.org/abs/2403.03954, code: https://github.com/YanjieZe/3D-Diffusion-Policy)
"""

import math
from collections import deque
from typing import Union

import einops
import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers.schedulers.scheduling_ddim import DDIMScheduler
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
from einops.layers.torch import Rearrange
from torch import Tensor

from lerobot.policies.dp3.configuration_dp3 import DP3Config
from lerobot.policies.pretrained import PreTrainedPolicy
from lerobot.policies.utils import (
    get_device_from_parameters,
    get_dtype_from_parameters,
    populate_queues,
)
from lerobot.utils.constants import ACTION, OBS_POINTCLOUD, OBS_STATE


class DP3Policy(PreTrainedPolicy):
    """3D Diffusion Policy (DP3) for point cloud-based visuomotor learning.

    DP3 uses point cloud observations as 3D representations and generates action
    sequences through a diffusion process. It combines PointNet-based encoding
    with a conditional U-Net for action generation.
    """

    config_class = DP3Config
    name = "dp3"

    def __init__(
        self,
        config: DP3Config,
        **kwargs,
    ):
        """Initialize DP3 policy.

        Args:
            config: Policy configuration instance.
            **kwargs: Additional arguments.
        """
        super().__init__(config)
        config.validate_features()
        self.config = config

        # Queues for caching observations and actions during rollout
        self._queues = None

        # Build the main diffusion model
        self.diffusion = DP3DiffusionModel(config)

        self.reset()

    def get_optim_params(self) -> dict:
        return self.diffusion.parameters()

    def reset(self):
        """Clear observation and action queues. Should be called on env.reset()."""
        self._queues = {
            OBS_STATE: deque(maxlen=self.config.n_obs_steps),
            OBS_POINTCLOUD: deque(maxlen=self.config.n_obs_steps),
            ACTION: deque(maxlen=self.config.n_action_steps),
        }

    @torch.no_grad()
    def predict_action_chunk(self, batch: dict[str, Tensor], noise: Tensor | None = None) -> Tensor:
        """Predict a chunk of actions given environment observations."""
        # Stack n latest observations from the queue
        batch = {k: torch.stack(list(self._queues[k]), dim=1) for k in batch if k in self._queues}
        actions = self.diffusion.generate_actions(batch, noise=noise)
        return actions

    @torch.no_grad()
    def select_action(self, batch: dict[str, Tensor], noise: Tensor | None = None) -> Tensor:
        """Select a single action given environment observations.

        This method handles caching a history of observations and an action trajectory
        generated by the underlying diffusion model.
        """
        # Remove action from batch if present (for offline evaluation)
        if ACTION in batch:
            batch.pop(ACTION)

        # Populate queues with new observations
        self._queues = populate_queues(self._queues, batch)

        if len(self._queues[ACTION]) == 0:
            actions = self.predict_action_chunk(batch, noise=noise)
            self._queues[ACTION].extend(actions.transpose(0, 1))

        action = self._queues[ACTION].popleft()
        return action

    def forward(self, batch: dict[str, Tensor]) -> tuple[Tensor, None]:
        """Run the batch through the model and compute the loss for training."""
        loss = self.diffusion.compute_loss(batch)
        return loss, None


def _make_noise_scheduler(name: str, **kwargs) -> DDPMScheduler | DDIMScheduler:
    """Factory for noise scheduler instances."""
    if name == "DDPM":
        return DDPMScheduler(**kwargs)
    elif name == "DDIM":
        return DDIMScheduler(**kwargs)
    else:
        raise ValueError(f"Unsupported noise scheduler type {name}")


# ============================================================================
# Point Cloud Encoders
# ============================================================================


class PointNetEncoderXYZ(nn.Module):
    """PointNet encoder for XYZ-only point clouds."""

    def __init__(
        self,
        in_channels: int = 3,
        out_channels: int = 256,
        use_layernorm: bool = False,
        final_norm: str = "none",
    ):
        super().__init__()
        block_channel = [64, 128, 256]

        self.mlp = nn.Sequential(
            nn.Linear(in_channels, block_channel[0]),
            nn.LayerNorm(block_channel[0]) if use_layernorm else nn.Identity(),
            nn.ReLU(),
            nn.Linear(block_channel[0], block_channel[1]),
            nn.LayerNorm(block_channel[1]) if use_layernorm else nn.Identity(),
            nn.ReLU(),
            nn.Linear(block_channel[1], block_channel[2]),
            nn.LayerNorm(block_channel[2]) if use_layernorm else nn.Identity(),
            nn.ReLU(),
        )

        if final_norm == "layernorm":
            self.final_projection = nn.Sequential(
                nn.Linear(block_channel[-1], out_channels),
                nn.LayerNorm(out_channels),
            )
        elif final_norm == "none":
            self.final_projection = nn.Linear(block_channel[-1], out_channels)
        else:
            raise NotImplementedError(f"final_norm: {final_norm}")

    def forward(self, x: Tensor) -> Tensor:
        """Forward pass.

        Args:
            x: Point cloud tensor of shape (B, N, 3).

        Returns:
            Feature tensor of shape (B, out_channels).
        """
        x = self.mlp(x)
        x = torch.max(x, 1)[0]  # Global max pooling
        x = self.final_projection(x)
        return x


class PointNetEncoderXYZRGB(nn.Module):
    """PointNet encoder for XYZRGB point clouds."""

    def __init__(
        self,
        in_channels: int = 6,
        out_channels: int = 256,
        use_layernorm: bool = False,
        final_norm: str = "none",
    ):
        super().__init__()
        block_channel = [64, 128, 256, 512]

        self.mlp = nn.Sequential(
            nn.Linear(in_channels, block_channel[0]),
            nn.LayerNorm(block_channel[0]) if use_layernorm else nn.Identity(),
            nn.ReLU(),
            nn.Linear(block_channel[0], block_channel[1]),
            nn.LayerNorm(block_channel[1]) if use_layernorm else nn.Identity(),
            nn.ReLU(),
            nn.Linear(block_channel[1], block_channel[2]),
            nn.LayerNorm(block_channel[2]) if use_layernorm else nn.Identity(),
            nn.ReLU(),
            nn.Linear(block_channel[2], block_channel[3]),
        )

        if final_norm == "layernorm":
            self.final_projection = nn.Sequential(
                nn.Linear(block_channel[-1], out_channels),
                nn.LayerNorm(out_channels),
            )
        elif final_norm == "none":
            self.final_projection = nn.Linear(block_channel[-1], out_channels)
        else:
            raise NotImplementedError(f"final_norm: {final_norm}")

    def forward(self, x: Tensor) -> Tensor:
        """Forward pass.

        Args:
            x: Point cloud tensor of shape (B, N, 6).

        Returns:
            Feature tensor of shape (B, out_channels).
        """
        x = self.mlp(x)
        x = torch.max(x, 1)[0]  # Global max pooling
        x = self.final_projection(x)
        return x


class DP3Encoder(nn.Module):
    """Combined encoder for point cloud and state observations."""

    def __init__(self, config: DP3Config):
        super().__init__()
        self.config = config

        # Get state dimension from config
        state_dim = config.robot_state_feature.shape[0]

        # Get point cloud dimension
        pc_feature = config.pointcloud_feature
        if pc_feature is None:
            raise ValueError("DP3Encoder requires pointcloud feature in config")

        # Determine input channels for point cloud
        if config.use_pc_color:
            pc_in_channels = 6
            self.pointcloud_encoder = PointNetEncoderXYZRGB(
                in_channels=pc_in_channels,
                out_channels=config.encoder_output_dim,
                use_layernorm=config.pointnet_use_layernorm,
                final_norm=config.pointnet_final_norm,
            )
        else:
            pc_in_channels = 3
            self.pointcloud_encoder = PointNetEncoderXYZ(
                in_channels=pc_in_channels,
                out_channels=config.encoder_output_dim,
                use_layernorm=config.pointnet_use_layernorm,
                final_norm=config.pointnet_final_norm,
            )

        # State MLP
        state_mlp_size = config.state_mlp_size
        if len(state_mlp_size) == 0:
            raise RuntimeError("State mlp size is empty")
        elif len(state_mlp_size) == 1:
            net_arch = []
        else:
            net_arch = list(state_mlp_size[:-1])
        output_dim = state_mlp_size[-1]

        state_mlp_layers = []
        in_dim = state_dim
        for hidden_dim in net_arch:
            state_mlp_layers.extend([nn.Linear(in_dim, hidden_dim), nn.ReLU()])
            in_dim = hidden_dim
        state_mlp_layers.append(nn.Linear(in_dim, output_dim))

        self.state_mlp = nn.Sequential(*state_mlp_layers)
        self.output_dim = config.encoder_output_dim + output_dim

    def forward(self, obs_dict: dict[str, Tensor]) -> Tensor:
        """Forward pass.

        Args:
            obs_dict: Dictionary containing 'observation.state' and 'observation.pointcloud'.

        Returns:
            Combined feature tensor of shape (B, output_dim).
        """
        point_cloud = obs_dict[OBS_POINTCLOUD]

        # Only use XYZ if not using color
        if not self.config.use_pc_color:
            point_cloud = point_cloud[..., :3]

        pc_feat = self.pointcloud_encoder(point_cloud)

        state = obs_dict[OBS_STATE]
        state_feat = self.state_mlp(state)

        return torch.cat([pc_feat, state_feat], dim=-1)

    def output_shape(self) -> int:
        return self.output_dim


# ============================================================================
# U-Net Components
# ============================================================================


class SinusoidalPosEmb(nn.Module):
    """1D sinusoidal positional embeddings."""

    def __init__(self, dim: int):
        super().__init__()
        self.dim = dim

    def forward(self, x: Tensor) -> Tensor:
        device = x.device
        half_dim = self.dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
        emb = x[:, None] * emb[None, :]
        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
        return emb


class Downsample1d(nn.Module):
    """1D downsampling layer."""

    def __init__(self, dim: int):
        super().__init__()
        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)

    def forward(self, x: Tensor) -> Tensor:
        return self.conv(x)


class Upsample1d(nn.Module):
    """1D upsampling layer."""

    def __init__(self, dim: int):
        super().__init__()
        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)

    def forward(self, x: Tensor) -> Tensor:
        return self.conv(x)


class Conv1dBlock(nn.Module):
    """Conv1d --> GroupNorm --> Mish."""

    def __init__(self, inp_channels: int, out_channels: int, kernel_size: int, n_groups: int = 8):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),
            nn.GroupNorm(n_groups, out_channels),
            nn.Mish(),
        )

    def forward(self, x: Tensor) -> Tensor:
        return self.block(x)


class CrossAttention(nn.Module):
    """Cross-attention module for conditioning."""

    def __init__(self, in_dim: int, cond_dim: int, out_dim: int):
        super().__init__()
        self.query_proj = nn.Linear(in_dim, out_dim)
        self.key_proj = nn.Linear(cond_dim, out_dim)
        self.value_proj = nn.Linear(cond_dim, out_dim)

    def forward(self, x: Tensor, cond: Tensor) -> Tensor:
        """Forward pass.

        Args:
            x: Query tensor of shape (B, T, in_dim).
            cond: Key/value tensor of shape (B, T_cond, cond_dim).

        Returns:
            Attended tensor of shape (B, T, out_dim).
        """
        query = self.query_proj(x)
        key = self.key_proj(cond)
        value = self.value_proj(cond)

        attn_weights = torch.matmul(query, key.transpose(-2, -1))
        attn_weights = F.softmax(attn_weights, dim=-1)

        return torch.matmul(attn_weights, value)


class ConditionalResidualBlock1D(nn.Module):
    """ResNet-style 1D convolutional block with FiLM modulation for conditioning."""

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        cond_dim: int,
        kernel_size: int = 3,
        n_groups: int = 8,
        condition_type: str = "film",
    ):
        super().__init__()

        self.blocks = nn.ModuleList(
            [
                Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),
                Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),
            ]
        )

        self.condition_type = condition_type
        self.out_channels = out_channels

        if condition_type == "film":
            cond_channels = out_channels * 2
            self.cond_encoder = nn.Sequential(
                nn.Mish(),
                nn.Linear(cond_dim, cond_channels),
                Rearrange("batch t -> batch t 1"),
            )
        elif condition_type == "add":
            self.cond_encoder = nn.Sequential(
                nn.Mish(),
                nn.Linear(cond_dim, out_channels),
                Rearrange("batch t -> batch t 1"),
            )
        elif condition_type == "cross_attention_add":
            self.cond_encoder = CrossAttention(in_channels, cond_dim, out_channels)
        elif condition_type == "cross_attention_film":
            cond_channels = out_channels * 2
            self.cond_encoder = CrossAttention(in_channels, cond_dim, cond_channels)
        elif condition_type == "mlp_film":
            cond_channels = out_channels * 2
            self.cond_encoder = nn.Sequential(
                nn.Mish(),
                nn.Linear(cond_dim, cond_dim),
                nn.Mish(),
                nn.Linear(cond_dim, cond_channels),
                Rearrange("batch t -> batch t 1"),
            )
        else:
            raise NotImplementedError(f"condition_type {condition_type} not implemented")

        self.residual_conv = (
            nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()
        )

    def forward(self, x: Tensor, cond: Tensor | None = None) -> Tensor:
        """Forward pass.

        Args:
            x: Input tensor of shape (B, in_channels, T).
            cond: Conditioning tensor of shape (B, cond_dim) or (B, T_cond, cond_dim).

        Returns:
            Output tensor of shape (B, out_channels, T).
        """
        out = self.blocks[0](x)

        if cond is not None:
            if self.condition_type == "film":
                embed = self.cond_encoder(cond)
                embed = embed.reshape(embed.shape[0], 2, self.out_channels, 1)
                scale = embed[:, 0, ...]
                bias = embed[:, 1, ...]
                out = scale * out + bias
            elif self.condition_type == "add":
                embed = self.cond_encoder(cond)
                out = out + embed
            elif self.condition_type == "cross_attention_add":
                embed = self.cond_encoder(x.permute(0, 2, 1), cond)
                embed = embed.permute(0, 2, 1)
                out = out + embed
            elif self.condition_type == "cross_attention_film":
                embed = self.cond_encoder(x.permute(0, 2, 1), cond)
                embed = embed.permute(0, 2, 1)
                embed = embed.reshape(embed.shape[0], 2, self.out_channels, -1)
                scale = embed[:, 0, ...]
                bias = embed[:, 1, ...]
                out = scale * out + bias
            elif self.condition_type == "mlp_film":
                embed = self.cond_encoder(cond)
                embed = embed.reshape(embed.shape[0], 2, self.out_channels, -1)
                scale = embed[:, 0, ...]
                bias = embed[:, 1, ...]
                out = scale * out + bias

        out = self.blocks[1](out)
        out = out + self.residual_conv(x)
        return out


class DP3ConditionalUnet1D(nn.Module):
    """1D conditional U-Net for DP3 diffusion model."""

    def __init__(self, config: DP3Config, global_cond_dim: int):
        super().__init__()
        self.config = config

        action_dim = config.action_feature.shape[0]
        input_dim = action_dim

        all_dims = [input_dim] + list(config.down_dims)
        start_dim = config.down_dims[0]

        # Diffusion step encoder
        dsed = config.diffusion_step_embed_dim
        self.diffusion_step_encoder = nn.Sequential(
            SinusoidalPosEmb(dsed),
            nn.Linear(dsed, dsed * 4),
            nn.Mish(),
            nn.Linear(dsed * 4, dsed),
        )

        cond_dim = dsed + global_cond_dim

        in_out = list(zip(all_dims[:-1], all_dims[1:], strict=True))

        # Down modules
        self.down_modules = nn.ModuleList([])
        for ind, (dim_in, dim_out) in enumerate(in_out):
            is_last = ind >= (len(in_out) - 1)
            self.down_modules.append(
                nn.ModuleList(
                    [
                        ConditionalResidualBlock1D(
                            dim_in,
                            dim_out,
                            cond_dim=cond_dim,
                            kernel_size=config.kernel_size,
                            n_groups=config.n_groups,
                            condition_type=config.condition_type,
                        ),
                        ConditionalResidualBlock1D(
                            dim_out,
                            dim_out,
                            cond_dim=cond_dim,
                            kernel_size=config.kernel_size,
                            n_groups=config.n_groups,
                            condition_type=config.condition_type,
                        ),
                        Downsample1d(dim_out) if not is_last else nn.Identity(),
                    ]
                )
            )

        # Mid modules
        mid_dim = all_dims[-1]
        self.mid_modules = nn.ModuleList(
            [
                ConditionalResidualBlock1D(
                    mid_dim,
                    mid_dim,
                    cond_dim=cond_dim,
                    kernel_size=config.kernel_size,
                    n_groups=config.n_groups,
                    condition_type=config.condition_type,
                ),
                ConditionalResidualBlock1D(
                    mid_dim,
                    mid_dim,
                    cond_dim=cond_dim,
                    kernel_size=config.kernel_size,
                    n_groups=config.n_groups,
                    condition_type=config.condition_type,
                ),
            ]
        )

        # Up modules
        self.up_modules = nn.ModuleList([])
        for ind, (dim_out, dim_in) in enumerate(reversed(in_out[1:])):
            is_last = ind >= (len(in_out) - 1)
            self.up_modules.append(
                nn.ModuleList(
                    [
                        ConditionalResidualBlock1D(
                            dim_in * 2,
                            dim_out,
                            cond_dim=cond_dim,
                            kernel_size=config.kernel_size,
                            n_groups=config.n_groups,
                            condition_type=config.condition_type,
                        ),
                        ConditionalResidualBlock1D(
                            dim_out,
                            dim_out,
                            cond_dim=cond_dim,
                            kernel_size=config.kernel_size,
                            n_groups=config.n_groups,
                            condition_type=config.condition_type,
                        ),
                        Upsample1d(dim_out) if not is_last else nn.Identity(),
                    ]
                )
            )

        # Final convolution
        self.final_conv = nn.Sequential(
            Conv1dBlock(start_dim, start_dim, kernel_size=config.kernel_size),
            nn.Conv1d(start_dim, action_dim, 1),
        )

        self.use_down_condition = config.use_down_condition
        self.use_mid_condition = config.use_mid_condition
        self.use_up_condition = config.use_up_condition

    def forward(
        self,
        sample: Tensor,
        timestep: Union[Tensor, float, int],
        global_cond: Tensor | None = None,
    ) -> Tensor:
        """Forward pass.

        Args:
            sample: Noisy action trajectory of shape (B, T, action_dim).
            timestep: Diffusion timestep(s).
            global_cond: Global conditioning of shape (B, global_cond_dim) or (B, T, global_cond_dim).

        Returns:
            Predicted noise or sample of shape (B, T, action_dim).
        """
        # Rearrange to (B, action_dim, T) for 1D convolutions
        x = einops.rearrange(sample, "b h t -> b t h")

        # Process timestep
        if not torch.is_tensor(timestep):
            timestep = torch.tensor([timestep], dtype=torch.long, device=sample.device)
        elif torch.is_tensor(timestep) and len(timestep.shape) == 0:
            timestep = timestep[None].to(sample.device)
        timestep = timestep.expand(sample.shape[0])

        timestep_embed = self.diffusion_step_encoder(timestep)

        # Prepare global feature
        if global_cond is not None:
            if "cross_attention" in self.config.condition_type:
                timestep_embed = timestep_embed.unsqueeze(1).expand(-1, global_cond.shape[1], -1)
            global_feature = torch.cat([timestep_embed, global_cond], dim=-1)
        else:
            global_feature = timestep_embed

        # Encoder path
        h = []
        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):
            if self.use_down_condition:
                x = resnet(x, global_feature)
                x = resnet2(x, global_feature)
            else:
                x = resnet(x, None)
                x = resnet2(x, None)
            h.append(x)
            x = downsample(x)

        # Middle
        for mid_module in self.mid_modules:
            if self.use_mid_condition:
                x = mid_module(x, global_feature)
            else:
                x = mid_module(x, None)

        # Decoder path
        for resnet, resnet2, upsample in self.up_modules:
            x = torch.cat((x, h.pop()), dim=1)
            if self.use_up_condition:
                x = resnet(x, global_feature)
                x = resnet2(x, global_feature)
            else:
                x = resnet(x, None)
                x = resnet2(x, None)
            x = upsample(x)

        x = self.final_conv(x)

        # Rearrange back to (B, T, action_dim)
        x = einops.rearrange(x, "b t h -> b h t")
        return x


# ============================================================================
# Mask Generator
# ============================================================================


class LowdimMaskGenerator(nn.Module):
    """Generates masks for low-dimensional conditioning."""

    def __init__(
        self,
        action_dim: int,
        obs_dim: int = 0,
        max_n_obs_steps: int = 2,
        fix_obs_steps: bool = True,
        action_visible: bool = False,
    ):
        super().__init__()
        self._dummy_param = nn.Parameter(torch.empty(0))
        self.action_dim = action_dim
        self.obs_dim = obs_dim
        self.max_n_obs_steps = max_n_obs_steps
        self.fix_obs_steps = fix_obs_steps
        self.action_visible = action_visible

    @property
    def device(self):
        return self._dummy_param.device

    @torch.no_grad()
    def forward(self, shape: tuple) -> Tensor:
        """Generate mask.

        Args:
            shape: Shape of the trajectory (B, T, D).

        Returns:
            Boolean mask tensor of shape (B, T, D).
        """
        device = self.device
        B, T, D = shape
        assert D == (self.action_dim + self.obs_dim)

        dim_mask = torch.zeros(size=shape, dtype=torch.bool, device=device)
        is_action_dim = dim_mask.clone()
        is_action_dim[..., : self.action_dim] = True
        is_obs_dim = ~is_action_dim

        if self.fix_obs_steps:
            obs_steps = torch.full((B,), fill_value=self.max_n_obs_steps, device=device)
        else:
            obs_steps = torch.randint(low=1, high=self.max_n_obs_steps + 1, size=(B,), device=device)

        steps = torch.arange(0, T, device=device).reshape(1, T).expand(B, T)
        obs_mask = (steps.T < obs_steps).T.reshape(B, T, 1).expand(B, T, D)
        obs_mask = obs_mask & is_obs_dim

        if self.action_visible:
            action_steps = torch.maximum(
                obs_steps - 1, torch.tensor(0, dtype=obs_steps.dtype, device=obs_steps.device)
            )
            action_mask = (steps.T < action_steps).T.reshape(B, T, 1).expand(B, T, D)
            action_mask = action_mask & is_action_dim
            return obs_mask | action_mask

        return obs_mask


# ============================================================================
# Main Diffusion Model
# ============================================================================


class DP3DiffusionModel(nn.Module):
    """Main DP3 diffusion model combining encoder and U-Net."""

    def __init__(self, config: DP3Config):
        super().__init__()
        self.config = config

        # Build observation encoder
        self.obs_encoder = DP3Encoder(config)
        obs_feature_dim = self.obs_encoder.output_shape()

        # Compute global conditioning dimension
        if config.obs_as_global_cond:
            if "cross_attention" in config.condition_type:
                global_cond_dim = obs_feature_dim
            else:
                global_cond_dim = obs_feature_dim * config.n_obs_steps
        else:
            global_cond_dim = 0

        # Build U-Net
        self.unet = DP3ConditionalUnet1D(config, global_cond_dim=global_cond_dim)

        # Build noise scheduler
        self.noise_scheduler = _make_noise_scheduler(
            config.noise_scheduler_type,
            num_train_timesteps=config.num_train_timesteps,
            beta_start=config.beta_start,
            beta_end=config.beta_end,
            beta_schedule=config.beta_schedule,
            clip_sample=config.clip_sample,
            clip_sample_range=config.clip_sample_range,
            prediction_type=config.prediction_type,
        )

        if config.num_inference_steps is None:
            self.num_inference_steps = self.noise_scheduler.config.num_train_timesteps
        else:
            self.num_inference_steps = config.num_inference_steps

        # Mask generator for inpainting-style conditioning (when not using global cond)
        action_dim = config.action_feature.shape[0]
        self.mask_generator = LowdimMaskGenerator(
            action_dim=action_dim,
            obs_dim=0 if config.obs_as_global_cond else obs_feature_dim,
            max_n_obs_steps=config.n_obs_steps,
            fix_obs_steps=True,
            action_visible=False,
        )

        self.obs_feature_dim = obs_feature_dim
        self.action_dim = action_dim

    def _prepare_global_conditioning(self, batch: dict[str, Tensor]) -> Tensor:
        """Encode observations and prepare global conditioning."""
        batch_size, n_obs_steps = batch[OBS_STATE].shape[:2]

        # Reshape observations for encoding: (B, T, ...) -> (B*T, ...)
        obs_dict = {
            OBS_STATE: batch[OBS_STATE].reshape(-1, *batch[OBS_STATE].shape[2:]),
            OBS_POINTCLOUD: batch[OBS_POINTCLOUD].reshape(-1, *batch[OBS_POINTCLOUD].shape[2:]),
        }

        obs_features = self.obs_encoder(obs_dict)

        if "cross_attention" in self.config.condition_type:
            # Treat as sequence for cross-attention
            global_cond = obs_features.reshape(batch_size, n_obs_steps, -1)
        else:
            # Concatenate temporal features
            global_cond = obs_features.reshape(batch_size, -1)

        return global_cond

    def conditional_sample(
        self,
        batch_size: int,
        global_cond: Tensor | None = None,
        generator: torch.Generator | None = None,
        noise: Tensor | None = None,
    ) -> Tensor:
        """Sample actions from the diffusion model.

        Args:
            batch_size: Number of samples to generate.
            global_cond: Global conditioning tensor.
            generator: Random number generator for reproducibility.
            noise: Initial noise (optional).

        Returns:
            Sampled action trajectory.
        """
        device = get_device_from_parameters(self)
        dtype = get_dtype_from_parameters(self)

        # Sample prior
        if noise is not None:
            sample = noise
        else:
            sample = torch.randn(
                size=(batch_size, self.config.horizon, self.action_dim),
                dtype=dtype,
                device=device,
                generator=generator,
            )

        self.noise_scheduler.set_timesteps(self.num_inference_steps)

        for t in self.noise_scheduler.timesteps:
            model_output = self.unet(
                sample,
                torch.full(sample.shape[:1], t, dtype=torch.long, device=sample.device),
                global_cond=global_cond,
            )
            sample = self.noise_scheduler.step(model_output, t, sample, generator=generator).prev_sample

        return sample

    def generate_actions(self, batch: dict[str, Tensor], noise: Tensor | None = None) -> Tensor:
        """Generate actions from observations.

        Args:
            batch: Dictionary containing observations with temporal dimension.
            noise: Optional initial noise for the diffusion process.

        Returns:
            Generated action sequence.
        """
        batch_size, n_obs_steps = batch[OBS_STATE].shape[:2]
        assert n_obs_steps == self.config.n_obs_steps

        global_cond = self._prepare_global_conditioning(batch)

        actions = self.conditional_sample(batch_size, global_cond=global_cond, noise=noise)

        # Extract n_action_steps worth of actions
        start = n_obs_steps - 1
        end = start + self.config.n_action_steps
        actions = actions[:, start:end]

        return actions

    def compute_loss(self, batch: dict[str, Tensor]) -> Tensor:
        """Compute diffusion training loss.

        Args:
            batch: Training batch containing observations and actions.

        Returns:
            Loss scalar.
        """
        assert set(batch).issuperset({OBS_STATE, OBS_POINTCLOUD, ACTION, "action_is_pad"})

        n_obs_steps = batch[OBS_STATE].shape[1]
        horizon = batch[ACTION].shape[1]
        assert horizon == self.config.horizon
        assert n_obs_steps == self.config.n_obs_steps

        global_cond = self._prepare_global_conditioning(batch)

        # Forward diffusion
        trajectory = batch[ACTION]
        eps = torch.randn(trajectory.shape, device=trajectory.device)
        timesteps = torch.randint(
            low=0,
            high=self.noise_scheduler.config.num_train_timesteps,
            size=(trajectory.shape[0],),
            device=trajectory.device,
        ).long()

        noisy_trajectory = self.noise_scheduler.add_noise(trajectory, eps, timesteps)

        # Predict
        pred = self.unet(noisy_trajectory, timesteps, global_cond=global_cond)

        # Compute target
        if self.config.prediction_type == "epsilon":
            target = eps
        elif self.config.prediction_type == "sample":
            target = batch[ACTION]
        elif self.config.prediction_type == "v_prediction":
            # v-prediction: v = alpha * noise - sigma * sample
            alphas_cumprod = self.noise_scheduler.alphas_cumprod.to(trajectory.device)
            alpha_t = alphas_cumprod[timesteps].view(-1, 1, 1)
            sigma_t = torch.sqrt(1 - alpha_t)
            target = alpha_t.sqrt() * eps - sigma_t * trajectory
        else:
            raise ValueError(f"Unsupported prediction type {self.config.prediction_type}")

        loss = F.mse_loss(pred, target, reduction="none")

        # Mask loss for padded actions
        if self.config.do_mask_loss_for_padding:
            if "action_is_pad" not in batch:
                raise ValueError(
                    "You need to provide 'action_is_pad' in the batch when "
                    f"{self.config.do_mask_loss_for_padding=}."
                )
            in_episode_bound = ~batch["action_is_pad"]
            loss = loss * in_episode_bound.unsqueeze(-1)

        return loss.mean()
